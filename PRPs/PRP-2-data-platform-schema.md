# PRP-2: Data Platform — Schema + Migrations

## Goal

Create a mini-warehouse schema enabling retail demand forecasting with exogenous drivers. Implement SQLAlchemy 2.0 ORM models and Alembic migrations for:

- **Dimension Tables**: `store`, `product`, `calendar`
- **Fact Tables**: `sales_daily` (required), `price_history`, `promotion`, `inventory_snapshot_daily`
- **Optional Tables** (stub-ready): `sales_txn`, `weather_daily`, `traffic_daily`

**End State:** A complete database schema deployed via Alembic with:
- All dimension and fact tables with proper relationships
- Grain-protection via unique constraints (`sales_daily`: date + store_id + product_id)
- Optimized indexes for time-range + store/product filtering
- Type-safe SQLAlchemy 2.0 models using `Mapped[]` and `mapped_column()`
- All validation gates passing (ruff, mypy, pyright, pytest)

---

## Why

- **Foundation for Forecasting**: All ForecastOps features (INITIAL-3 through INITIAL-9) depend on this schema
- **Grain Protection**: Prevent data quality issues with explicit unique constraints at DB level
- **Query Performance**: Indexes optimized for time-series queries (date ranges + store/product filtering)
- **Reproducibility**: Migrations enable consistent schema across environments
- **Type Safety**: SQLAlchemy 2.0 patterns provide IDE support and catch errors at development time

---

## What

### Success Criteria

- [ ] Alembic migration creates all tables: `store`, `product`, `calendar`, `sales_daily`, `price_history`, `promotion`, `inventory_snapshot_daily`
- [ ] `sales_daily` has unique constraint on `(date, store_id, product_id)`
- [ ] All fact tables have foreign keys to dimension tables
- [ ] Composite indexes exist for common query patterns (date range + store/product)
- [ ] `uv run alembic upgrade head` creates all tables successfully
- [ ] `uv run alembic downgrade -1 && uv run alembic upgrade head` works (reversible)
- [ ] All models pass `uv run mypy app/` with zero errors
- [ ] All models pass `uv run pyright app/` with zero errors
- [ ] Unit tests validate model relationships and constraints
- [ ] Integration tests verify constraint enforcement (unique, foreign key)
- [ ] Example files created: `examples/schema/README.md`, `examples/queries/kpi_sales.sql`, `examples/queries/exog_join.sql`

---

## All Needed Context

### Documentation & References

```yaml
# MUST READ - Include these in your context window
- url: https://docs.sqlalchemy.org/en/20/orm/quickstart.html
  why: SQLAlchemy 2.0 ORM patterns - DeclarativeBase, Mapped[], mapped_column()
  critical: Use `Mapped[type]` for all column annotations, `mapped_column()` for all columns

- url: https://docs.sqlalchemy.org/en/20/orm/mapper_config.html
  why: Advanced ORM configuration including relationship patterns
  critical: Use `relationship()` with `back_populates` for bidirectional relations

- url: https://alembic.sqlalchemy.org/en/latest/ops.html
  why: Alembic migration operations for create_table, create_index, create_unique_constraint
  critical: Use op.create_index() for composite indexes, specify columns as list of strings

- url: https://alembic.sqlalchemy.org/en/latest/naming.html
  why: Constraint naming conventions for reproducible migrations
  critical: Named constraints enable proper downgrade operations

- url: https://alembic.sqlalchemy.org/en/latest/autogenerate.html
  why: Autogenerate capabilities and limitations
  critical: Always review autogenerated migrations - not intended to be perfect

- url: https://www.analyticsvidhya.com/blog/2025/10/retail-demand-forecasting/
  why: Retail demand forecasting data requirements and feature engineering patterns
  critical: Essential data = daily sales by store+SKU, prices, discounts, calendar

- docfile: app/core/database.py
  why: Existing Base class and async session patterns to follow

- docfile: app/shared/models.py
  why: TimestampMixin to inherit for created_at/updated_at columns

- docfile: alembic/env.py
  why: Async migration environment already configured

- docfile: docs/validation/logging-standard.md
  why: Event naming for migration and database operation logging

- docfile: CLAUDE.md
  why: All project conventions, type safety requirements, vertical slice architecture
```

### Current Codebase Tree

```bash
app/
├── __init__.py
├── main.py                 # FastAPI entry point
├── core/
│   ├── __init__.py
│   ├── config.py           # Pydantic Settings
│   ├── database.py         # Base class, get_db(), async session
│   ├── exceptions.py       # Custom exceptions
│   ├── health.py           # Health endpoints
│   ├── logging.py          # Structlog configuration
│   ├── middleware.py       # Request ID middleware
│   └── tests/              # Core module tests
├── shared/
│   ├── __init__.py
│   ├── models.py           # TimestampMixin
│   ├── schemas.py          # Pagination, error schemas
│   └── utils.py            # Common utilities
└── features/
    └── __init__.py         # Empty - ready for vertical slices

alembic/
├── env.py                  # Async migration runner (already configured)
├── script.py.mako          # Migration template
└── versions/
    └── .gitkeep            # Empty - no migrations yet
```

### Desired Codebase Tree (files to be added)

```bash
app/
├── features/
│   └── data_platform/              # NEW: Data platform vertical slice
│       ├── __init__.py             # Module exports
│       ├── models.py               # SQLAlchemy ORM models (all tables)
│       ├── schemas.py              # Pydantic schemas for data validation
│       └── tests/
│           ├── __init__.py
│           ├── conftest.py         # Feature-specific fixtures
│           ├── test_models.py      # Model relationship tests
│           └── test_constraints.py # Constraint enforcement tests (integration)

alembic/
└── versions/
    └── 0001_create_data_platform_tables.py  # NEW: Baseline migration

examples/
├── schema/
│   └── README.md                   # NEW: Table grains + keys + rationale
└── queries/
    ├── kpi_sales.sql               # NEW: KPI query shapes
    └── exog_join.sql               # NEW: Join pattern examples
```

### Known Gotchas & Library Quirks

```python
# CRITICAL: SQLAlchemy 2.0 type annotations
# ❌ OLD: id = Column(Integer, primary_key=True)
# ✅ NEW: id: Mapped[int] = mapped_column(primary_key=True)

# CRITICAL: Optional columns use Union type or None
# ❌ WRONG: description: Mapped[str | None]  # This works but be explicit
# ✅ CORRECT: description: Mapped[str | None] = mapped_column(nullable=True)

# CRITICAL: Foreign keys must reference the TABLE name, not class name
# ❌ WRONG: store_id: Mapped[int] = mapped_column(ForeignKey("Store.id"))
# ✅ CORRECT: store_id: Mapped[int] = mapped_column(ForeignKey("store.id"))

# CRITICAL: Relationship back_populates must match attribute names exactly
# ✅ CORRECT:
class Store(Base):
    sales: Mapped[list["SalesDaily"]] = relationship(back_populates="store")
class SalesDaily(Base):
    store: Mapped["Store"] = relationship(back_populates="sales")

# CRITICAL: datetime columns MUST use timezone=True for PostgreSQL
# ❌ WRONG: date: Mapped[datetime] = mapped_column(Date)
# ✅ CORRECT: date: Mapped[date] = mapped_column(Date)  # Date type for date-only
# ✅ CORRECT: timestamp: Mapped[datetime] = mapped_column(DateTime(timezone=True))

# CRITICAL: Decimal for money/price - never use float
# ✅ CORRECT: price: Mapped[Decimal] = mapped_column(Numeric(10, 2))

# CRITICAL: Alembic composite unique constraint requires UniqueConstraint in __table_args__
# ✅ CORRECT:
class SalesDaily(Base):
    __table_args__ = (
        UniqueConstraint("date", "store_id", "product_id", name="uq_sales_daily_grain"),
    )

# CRITICAL: Index naming convention for maintainability
# Format: ix_{table}_{columns} for regular indexes
# Format: uq_{table}_{columns} for unique constraints
# Format: fk_{source_table}_{target_table} for foreign keys

# CRITICAL: Alembic import models in env.py for autogenerate to work
# In alembic/env.py, add:
from app.features.data_platform.models import *  # noqa: F401, F403

# CRITICAL: PostgreSQL unique constraint automatically creates index
# Don't create separate index for unique constraint columns
# https://github.com/sqlalchemy/alembic/issues/1511
```

---

## Implementation Blueprint

### Data Models and Structure

#### Entity-Relationship Overview

```
┌──────────────┐     ┌──────────────┐     ┌──────────────┐
│    Store     │     │   Product    │     │   Calendar   │
│──────────────│     │──────────────│     │──────────────│
│ id (PK)      │     │ id (PK)      │     │ date (PK)    │
│ code         │     │ sku          │     │ day_of_week  │
│ name         │     │ name         │     │ month        │
│ region       │     │ category     │     │ quarter      │
│ city         │     │ brand        │     │ year         │
│ store_type   │     │ base_price   │     │ is_holiday   │
│ created_at   │     │ base_cost    │     │ holiday_name │
│ updated_at   │     │ created_at   │     │ created_at   │
└──────┬───────┘     │ updated_at   │     │ updated_at   │
       │             └──────┬───────┘     └──────┬───────┘
       │                    │                    │
       ▼                    ▼                    ▼
┌─────────────────────────────────────────────────────────┐
│                     SalesDaily                          │
│─────────────────────────────────────────────────────────│
│ id (PK)                                                 │
│ date (FK→Calendar) ─────────────────────────────────────│
│ store_id (FK→Store)                                     │
│ product_id (FK→Product)                                 │
│ quantity                                                │
│ unit_price                                              │
│ total_amount                                            │
│ UNIQUE(date, store_id, product_id) ← GRAIN PROTECTION  │
└─────────────────────────────────────────────────────────┘

┌──────────────┐     ┌──────────────┐     ┌─────────────────────┐
│ PriceHistory │     │  Promotion   │     │InventorySnapshotDaily│
│──────────────│     │──────────────│     │─────────────────────│
│ id (PK)      │     │ id (PK)      │     │ id (PK)             │
│ product_id   │     │ product_id   │     │ date                │
│ store_id     │     │ store_id     │     │ store_id            │
│ price        │     │ name         │     │ product_id          │
│ valid_from   │     │ discount_pct │     │ on_hand_qty         │
│ valid_to     │     │ start_date   │     │ on_order_qty        │
│ created_at   │     │ end_date     │     │ is_stockout         │
│ updated_at   │     │ created_at   │     │ created_at          │
└──────────────┘     │ updated_at   │     │ updated_at          │
                     └──────────────┘     └─────────────────────┘
```

#### SQLAlchemy 2.0 Model Patterns (app/features/data_platform/models.py)

```python
"""Data platform ORM models for retail forecasting mini-warehouse.

This module defines dimension and fact tables following star schema patterns:
- Dimensions: Store, Product, Calendar
- Facts: SalesDaily, PriceHistory, Promotion, InventorySnapshotDaily

Grain: SalesDaily uniquely keyed by (date, store_id, product_id).
"""

from datetime import date, datetime
from decimal import Decimal

from sqlalchemy import (
    CheckConstraint,
    Date,
    DateTime,
    ForeignKey,
    Index,
    Numeric,
    String,
    UniqueConstraint,
    func,
)
from sqlalchemy.orm import Mapped, mapped_column, relationship

from app.core.database import Base
from app.shared.models import TimestampMixin


# ============================================================================
# DIMENSION TABLES
# ============================================================================

class Store(TimestampMixin, Base):
    """Store dimension table.

    Attributes:
        id: Primary key.
        code: Unique store code (e.g., "S001").
        name: Store display name.
        region: Geographic region.
        city: City location.
        store_type: Store format (e.g., "supermarket", "express", "warehouse").
    """

    __tablename__ = "store"

    id: Mapped[int] = mapped_column(primary_key=True)
    code: Mapped[str] = mapped_column(String(20), unique=True, index=True)
    name: Mapped[str] = mapped_column(String(100))
    region: Mapped[str | None] = mapped_column(String(50))
    city: Mapped[str | None] = mapped_column(String(50))
    store_type: Mapped[str | None] = mapped_column(String(30))

    # Relationships (one-to-many)
    sales: Mapped[list["SalesDaily"]] = relationship(back_populates="store")
    price_history: Mapped[list["PriceHistory"]] = relationship(back_populates="store")
    promotions: Mapped[list["Promotion"]] = relationship(back_populates="store")
    inventory_snapshots: Mapped[list["InventorySnapshotDaily"]] = relationship(
        back_populates="store"
    )


class Product(TimestampMixin, Base):
    """Product dimension table.

    Attributes:
        id: Primary key.
        sku: Stock keeping unit (unique product identifier).
        name: Product display name.
        category: Product category.
        brand: Product brand.
        base_price: Standard retail price.
        base_cost: Standard cost/COGS.
    """

    __tablename__ = "product"

    id: Mapped[int] = mapped_column(primary_key=True)
    sku: Mapped[str] = mapped_column(String(50), unique=True, index=True)
    name: Mapped[str] = mapped_column(String(200))
    category: Mapped[str | None] = mapped_column(String(100), index=True)
    brand: Mapped[str | None] = mapped_column(String(100))
    base_price: Mapped[Decimal | None] = mapped_column(Numeric(10, 2))
    base_cost: Mapped[Decimal | None] = mapped_column(Numeric(10, 2))

    # Relationships (one-to-many)
    sales: Mapped[list["SalesDaily"]] = relationship(back_populates="product")
    price_history: Mapped[list["PriceHistory"]] = relationship(back_populates="product")
    promotions: Mapped[list["Promotion"]] = relationship(back_populates="product")
    inventory_snapshots: Mapped[list["InventorySnapshotDaily"]] = relationship(
        back_populates="product"
    )


class Calendar(TimestampMixin, Base):
    """Calendar dimension table for time-based analysis.

    Uses date as primary key (no surrogate key needed).

    Attributes:
        date: Calendar date (primary key).
        day_of_week: 0=Monday, 6=Sunday.
        month: Month number (1-12).
        quarter: Quarter number (1-4).
        year: Year (e.g., 2024).
        is_holiday: Whether this date is a holiday.
        holiday_name: Name of the holiday (if applicable).
    """

    __tablename__ = "calendar"

    date: Mapped[date] = mapped_column(Date, primary_key=True)
    day_of_week: Mapped[int] = mapped_column()  # 0=Monday, 6=Sunday
    month: Mapped[int] = mapped_column()
    quarter: Mapped[int] = mapped_column()
    year: Mapped[int] = mapped_column(index=True)
    is_holiday: Mapped[bool] = mapped_column(default=False)
    holiday_name: Mapped[str | None] = mapped_column(String(100))

    # Relationships
    sales: Mapped[list["SalesDaily"]] = relationship(back_populates="calendar")
    inventory_snapshots: Mapped[list["InventorySnapshotDaily"]] = relationship(
        back_populates="calendar"
    )

    __table_args__ = (
        CheckConstraint("day_of_week >= 0 AND day_of_week <= 6", name="ck_calendar_day_of_week"),
        CheckConstraint("month >= 1 AND month <= 12", name="ck_calendar_month"),
        CheckConstraint("quarter >= 1 AND quarter <= 4", name="ck_calendar_quarter"),
    )


# ============================================================================
# FACT TABLES
# ============================================================================

class SalesDaily(TimestampMixin, Base):
    """Daily sales fact table.

    CRITICAL: Grain is (date, store_id, product_id) - one row per store/product/day.
    Enforced by unique constraint for idempotent upserts.

    Attributes:
        id: Surrogate primary key.
        date: Sales date (FK to calendar).
        store_id: Store (FK to store).
        product_id: Product (FK to product).
        quantity: Units sold.
        unit_price: Price per unit at time of sale.
        total_amount: Total sales amount (quantity * unit_price).
    """

    __tablename__ = "sales_daily"

    id: Mapped[int] = mapped_column(primary_key=True)
    date: Mapped[date] = mapped_column(Date, ForeignKey("calendar.date"), index=True)
    store_id: Mapped[int] = mapped_column(ForeignKey("store.id"), index=True)
    product_id: Mapped[int] = mapped_column(ForeignKey("product.id"), index=True)
    quantity: Mapped[int] = mapped_column()
    unit_price: Mapped[Decimal] = mapped_column(Numeric(10, 2))
    total_amount: Mapped[Decimal] = mapped_column(Numeric(12, 2))

    # Relationships
    store: Mapped["Store"] = relationship(back_populates="sales")
    product: Mapped["Product"] = relationship(back_populates="sales")
    calendar: Mapped["Calendar"] = relationship(back_populates="sales")

    __table_args__ = (
        # GRAIN PROTECTION: Unique constraint prevents duplicate rows
        UniqueConstraint("date", "store_id", "product_id", name="uq_sales_daily_grain"),
        # Composite index for common query pattern: date range + store
        Index("ix_sales_daily_date_store", "date", "store_id"),
        # Composite index for date range + product
        Index("ix_sales_daily_date_product", "date", "product_id"),
        # Check constraint for data quality
        CheckConstraint("quantity >= 0", name="ck_sales_daily_quantity_positive"),
        CheckConstraint("unit_price >= 0", name="ck_sales_daily_price_positive"),
        CheckConstraint("total_amount >= 0", name="ck_sales_daily_amount_positive"),
    )


class PriceHistory(TimestampMixin, Base):
    """Price history fact table with validity windows.

    Tracks price changes over time with valid_from/valid_to windows.
    valid_to = NULL means currently active price.

    Attributes:
        id: Primary key.
        product_id: Product (FK).
        store_id: Store (FK) - NULL for chain-wide prices.
        price: Price during validity window.
        valid_from: Start of validity period.
        valid_to: End of validity period (NULL = current).
    """

    __tablename__ = "price_history"

    id: Mapped[int] = mapped_column(primary_key=True)
    product_id: Mapped[int] = mapped_column(ForeignKey("product.id"), index=True)
    store_id: Mapped[int | None] = mapped_column(ForeignKey("store.id"), index=True)
    price: Mapped[Decimal] = mapped_column(Numeric(10, 2))
    valid_from: Mapped[date] = mapped_column(Date, index=True)
    valid_to: Mapped[date | None] = mapped_column(Date)

    # Relationships
    product: Mapped["Product"] = relationship(back_populates="price_history")
    store: Mapped["Store | None"] = relationship(back_populates="price_history")

    __table_args__ = (
        Index("ix_price_history_product_validity", "product_id", "valid_from", "valid_to"),
        CheckConstraint("price >= 0", name="ck_price_history_price_positive"),
        CheckConstraint(
            "valid_to IS NULL OR valid_to >= valid_from",
            name="ck_price_history_valid_dates",
        ),
    )


class Promotion(TimestampMixin, Base):
    """Promotion fact table.

    Tracks promotional campaigns with discount mechanics.

    Attributes:
        id: Primary key.
        product_id: Product (FK).
        store_id: Store (FK) - NULL for chain-wide promos.
        name: Promotion name/description.
        discount_pct: Discount percentage (e.g., 0.15 for 15% off).
        discount_amount: Fixed discount amount (alternative to %).
        start_date: Promotion start date.
        end_date: Promotion end date.
    """

    __tablename__ = "promotion"

    id: Mapped[int] = mapped_column(primary_key=True)
    product_id: Mapped[int] = mapped_column(ForeignKey("product.id"), index=True)
    store_id: Mapped[int | None] = mapped_column(ForeignKey("store.id"), index=True)
    name: Mapped[str] = mapped_column(String(200))
    discount_pct: Mapped[Decimal | None] = mapped_column(Numeric(5, 4))
    discount_amount: Mapped[Decimal | None] = mapped_column(Numeric(10, 2))
    start_date: Mapped[date] = mapped_column(Date, index=True)
    end_date: Mapped[date] = mapped_column(Date)

    # Relationships
    product: Mapped["Product"] = relationship(back_populates="promotions")
    store: Mapped["Store | None"] = relationship(back_populates="promotions")

    __table_args__ = (
        Index("ix_promotion_product_dates", "product_id", "start_date", "end_date"),
        CheckConstraint("end_date >= start_date", name="ck_promotion_valid_dates"),
        CheckConstraint(
            "discount_pct IS NULL OR (discount_pct >= 0 AND discount_pct <= 1)",
            name="ck_promotion_discount_pct_range",
        ),
        CheckConstraint(
            "discount_amount IS NULL OR discount_amount >= 0",
            name="ck_promotion_discount_amount_positive",
        ),
    )


class InventorySnapshotDaily(TimestampMixin, Base):
    """Daily inventory snapshot fact table.

    Daily end-of-day inventory levels for stockout detection.

    Attributes:
        id: Primary key.
        date: Snapshot date (FK to calendar).
        store_id: Store (FK).
        product_id: Product (FK).
        on_hand_qty: Units on hand at end of day.
        on_order_qty: Units on order (incoming).
        is_stockout: True if on_hand_qty = 0.
    """

    __tablename__ = "inventory_snapshot_daily"

    id: Mapped[int] = mapped_column(primary_key=True)
    date: Mapped[date] = mapped_column(Date, ForeignKey("calendar.date"), index=True)
    store_id: Mapped[int] = mapped_column(ForeignKey("store.id"), index=True)
    product_id: Mapped[int] = mapped_column(ForeignKey("product.id"), index=True)
    on_hand_qty: Mapped[int] = mapped_column()
    on_order_qty: Mapped[int] = mapped_column(default=0)
    is_stockout: Mapped[bool] = mapped_column(default=False)

    # Relationships
    calendar: Mapped["Calendar"] = relationship(back_populates="inventory_snapshots")
    store: Mapped["Store"] = relationship(back_populates="inventory_snapshots")
    product: Mapped["Product"] = relationship(back_populates="inventory_snapshots")

    __table_args__ = (
        UniqueConstraint(
            "date", "store_id", "product_id", name="uq_inventory_snapshot_daily_grain"
        ),
        Index("ix_inventory_snapshot_date_store", "date", "store_id"),
        CheckConstraint("on_hand_qty >= 0", name="ck_inventory_on_hand_positive"),
        CheckConstraint("on_order_qty >= 0", name="ck_inventory_on_order_positive"),
    )
```

---

## Tasks (Ordered Implementation)

### Task 1: Create data_platform feature directory structure

**Files to create:**
```
app/features/data_platform/__init__.py
app/features/data_platform/models.py
app/features/data_platform/schemas.py
app/features/data_platform/tests/__init__.py
app/features/data_platform/tests/conftest.py
```

**Pseudocode:**
```python
# app/features/data_platform/__init__.py
"""Data platform feature for retail forecasting mini-warehouse."""

from app.features.data_platform.models import (
    Calendar,
    InventorySnapshotDaily,
    PriceHistory,
    Product,
    Promotion,
    SalesDaily,
    Store,
)

__all__ = [
    "Store",
    "Product",
    "Calendar",
    "SalesDaily",
    "PriceHistory",
    "Promotion",
    "InventorySnapshotDaily",
]
```

**Validation:**
```bash
# Verify directory structure
ls -la app/features/data_platform/
```

---

### Task 2: Implement dimension models (Store, Product, Calendar)

**File:** `app/features/data_platform/models.py`

Implement the three dimension tables as shown in the blueprint above. Key points:
- Use `TimestampMixin` from `app.shared.models`
- Use `Mapped[]` type annotations for all columns
- Use `mapped_column()` for all column definitions
- Add `relationship()` with `back_populates` for bidirectional navigation
- Add appropriate indexes on frequently queried columns

**Validation:**
```bash
uv run python -c "from app.features.data_platform.models import Store, Product, Calendar; print('Dimension models OK')"
uv run mypy app/features/data_platform/models.py
uv run pyright app/features/data_platform/models.py
```

---

### Task 3: Implement fact models (SalesDaily, PriceHistory, Promotion, InventorySnapshotDaily)

**File:** `app/features/data_platform/models.py` (append to existing)

Implement the four fact tables as shown in the blueprint. Key points:
- `SalesDaily` MUST have `UniqueConstraint("date", "store_id", "product_id")` for grain protection
- Use `Decimal` via `Numeric(10, 2)` for all monetary values
- Add `CheckConstraint` for data quality (positive quantities, valid date ranges)
- Add composite indexes for common query patterns

**Validation:**
```bash
uv run python -c "from app.features.data_platform.models import SalesDaily, PriceHistory, Promotion, InventorySnapshotDaily; print('Fact models OK')"
uv run mypy app/features/data_platform/models.py
uv run pyright app/features/data_platform/models.py
```

---

### Task 4: Update alembic/env.py to import models

**File:** `alembic/env.py`

**MODIFY:** Add import of data platform models so autogenerate detects them.

```python
# Add after existing imports in alembic/env.py
# Import all models for Alembic autogenerate detection
from app.features.data_platform import models as data_platform_models  # noqa: F401
```

**Validation:**
```bash
uv run alembic check  # Should report "New upgrade operations detected"
```

---

### Task 5: Generate Alembic baseline migration

**Command:**
```bash
uv run alembic revision --autogenerate -m "create_data_platform_tables"
```

**Post-generation review checklist:**
- [ ] All 7 tables created (store, product, calendar, sales_daily, price_history, promotion, inventory_snapshot_daily)
- [ ] Foreign keys reference correct tables
- [ ] Unique constraints have proper names (uq_*)
- [ ] Indexes have proper names (ix_*)
- [ ] Check constraints have proper names (ck_*)
- [ ] `downgrade()` function properly drops tables in reverse order

**Manual adjustments if needed:**
- Ensure indexes are created AFTER tables
- Ensure foreign keys reference existing tables
- Verify constraint names follow naming convention

**Validation:**
```bash
# Test migration up/down cycle
docker-compose up -d
sleep 5
uv run alembic upgrade head
uv run alembic downgrade -1
uv run alembic upgrade head
docker-compose down
```

---

### Task 6: Create Pydantic schemas for data validation

**File:** `app/features/data_platform/schemas.py`

```python
"""Pydantic schemas for data platform validation.

These schemas are used for API input/output validation,
not for ORM operations directly.
"""

from datetime import date
from decimal import Decimal

from pydantic import BaseModel, Field, field_validator


class StoreBase(BaseModel):
    """Base schema for store data."""

    code: str = Field(..., min_length=1, max_length=20)
    name: str = Field(..., min_length=1, max_length=100)
    region: str | None = Field(None, max_length=50)
    city: str | None = Field(None, max_length=50)
    store_type: str | None = Field(None, max_length=30)


class StoreCreate(StoreBase):
    """Schema for creating a new store."""

    pass


class StoreRead(StoreBase):
    """Schema for reading store data."""

    id: int

    model_config = {"from_attributes": True}


class ProductBase(BaseModel):
    """Base schema for product data."""

    sku: str = Field(..., min_length=1, max_length=50)
    name: str = Field(..., min_length=1, max_length=200)
    category: str | None = Field(None, max_length=100)
    brand: str | None = Field(None, max_length=100)
    base_price: Decimal | None = Field(None, ge=0, decimal_places=2)
    base_cost: Decimal | None = Field(None, ge=0, decimal_places=2)


class ProductCreate(ProductBase):
    """Schema for creating a new product."""

    pass


class ProductRead(ProductBase):
    """Schema for reading product data."""

    id: int

    model_config = {"from_attributes": True}


class SalesDailyBase(BaseModel):
    """Base schema for daily sales data."""

    date: date
    store_id: int = Field(..., gt=0)
    product_id: int = Field(..., gt=0)
    quantity: int = Field(..., ge=0)
    unit_price: Decimal = Field(..., ge=0, decimal_places=2)
    total_amount: Decimal = Field(..., ge=0, decimal_places=2)

    @field_validator("total_amount", mode="before")
    @classmethod
    def validate_total_amount(cls, v: Decimal, info) -> Decimal:
        """Validate total_amount matches quantity * unit_price."""
        # Allow validation to pass - business logic can verify
        return v


class SalesDailyCreate(SalesDailyBase):
    """Schema for creating daily sales record."""

    pass


class SalesDailyRead(SalesDailyBase):
    """Schema for reading daily sales data."""

    id: int

    model_config = {"from_attributes": True}
```

**Validation:**
```bash
uv run python -c "from app.features.data_platform.schemas import StoreCreate, ProductCreate, SalesDailyCreate; print('Schemas OK')"
uv run mypy app/features/data_platform/schemas.py
uv run pyright app/features/data_platform/schemas.py
```

---

### Task 7: Create unit tests for model definitions

**File:** `app/features/data_platform/tests/test_models.py`

```python
"""Tests for data platform ORM models."""

from datetime import date
from decimal import Decimal

from app.features.data_platform.models import (
    Calendar,
    InventorySnapshotDaily,
    PriceHistory,
    Product,
    Promotion,
    SalesDaily,
    Store,
)


class TestStoreModel:
    """Tests for Store model."""

    def test_store_tablename(self):
        """Store model should have correct table name."""
        assert Store.__tablename__ == "store"

    def test_store_has_required_columns(self):
        """Store model should have all required columns."""
        columns = {c.name for c in Store.__table__.columns}
        required = {"id", "code", "name", "region", "city", "store_type", "created_at", "updated_at"}
        assert required.issubset(columns)

    def test_store_code_is_unique(self):
        """Store code column should be unique."""
        code_col = Store.__table__.columns["code"]
        assert code_col.unique is True


class TestProductModel:
    """Tests for Product model."""

    def test_product_tablename(self):
        """Product model should have correct table name."""
        assert Product.__tablename__ == "product"

    def test_product_sku_is_unique(self):
        """Product SKU column should be unique."""
        sku_col = Product.__table__.columns["sku"]
        assert sku_col.unique is True

    def test_product_price_is_numeric(self):
        """Product base_price should be Numeric type."""
        price_col = Product.__table__.columns["base_price"]
        assert "NUMERIC" in str(price_col.type).upper()


class TestCalendarModel:
    """Tests for Calendar model."""

    def test_calendar_date_is_primary_key(self):
        """Calendar date should be primary key."""
        date_col = Calendar.__table__.columns["date"]
        assert date_col.primary_key is True


class TestSalesDailyModel:
    """Tests for SalesDaily model."""

    def test_sales_daily_tablename(self):
        """SalesDaily model should have correct table name."""
        assert SalesDaily.__tablename__ == "sales_daily"

    def test_sales_daily_has_grain_constraint(self):
        """SalesDaily should have unique constraint on grain."""
        constraints = [c.name for c in SalesDaily.__table__.constraints]
        assert "uq_sales_daily_grain" in constraints

    def test_sales_daily_has_foreign_keys(self):
        """SalesDaily should have foreign keys to dimensions."""
        fk_columns = {fk.column.table.name for fk in SalesDaily.__table__.foreign_keys}
        assert fk_columns == {"calendar", "store", "product"}

    def test_sales_daily_has_check_constraints(self):
        """SalesDaily should have check constraints for data quality."""
        constraints = [c.name for c in SalesDaily.__table__.constraints if hasattr(c, "name")]
        assert "ck_sales_daily_quantity_positive" in constraints
        assert "ck_sales_daily_price_positive" in constraints


class TestPriceHistoryModel:
    """Tests for PriceHistory model."""

    def test_price_history_has_validity_dates(self):
        """PriceHistory should have valid_from and valid_to columns."""
        columns = {c.name for c in PriceHistory.__table__.columns}
        assert "valid_from" in columns
        assert "valid_to" in columns


class TestPromotionModel:
    """Tests for Promotion model."""

    def test_promotion_has_discount_fields(self):
        """Promotion should have discount_pct and discount_amount."""
        columns = {c.name for c in Promotion.__table__.columns}
        assert "discount_pct" in columns
        assert "discount_amount" in columns


class TestInventorySnapshotDailyModel:
    """Tests for InventorySnapshotDaily model."""

    def test_inventory_has_grain_constraint(self):
        """InventorySnapshotDaily should have unique constraint on grain."""
        constraints = [c.name for c in InventorySnapshotDaily.__table__.constraints]
        assert "uq_inventory_snapshot_daily_grain" in constraints
```

**Validation:**
```bash
uv run pytest app/features/data_platform/tests/test_models.py -v
```

---

### Task 8: Create integration tests for constraint enforcement

**File:** `app/features/data_platform/tests/test_constraints.py`

```python
"""Integration tests for database constraint enforcement.

These tests require a running PostgreSQL database.
Mark with @pytest.mark.integration.
"""

import pytest
from sqlalchemy import select
from sqlalchemy.exc import IntegrityError
from sqlalchemy.ext.asyncio import AsyncSession

from app.features.data_platform.models import Calendar, Product, SalesDaily, Store


@pytest.mark.integration
class TestSalesDailyConstraints:
    """Integration tests for SalesDaily constraints."""

    async def test_unique_constraint_prevents_duplicates(
        self, db_session: AsyncSession, sample_store: Store, sample_product: Product, sample_calendar: Calendar
    ):
        """Inserting duplicate grain should raise IntegrityError."""
        from datetime import date
        from decimal import Decimal

        # First insert should succeed
        sale1 = SalesDaily(
            date=sample_calendar.date,
            store_id=sample_store.id,
            product_id=sample_product.id,
            quantity=10,
            unit_price=Decimal("9.99"),
            total_amount=Decimal("99.90"),
        )
        db_session.add(sale1)
        await db_session.commit()

        # Second insert with same grain should fail
        sale2 = SalesDaily(
            date=sample_calendar.date,
            store_id=sample_store.id,
            product_id=sample_product.id,
            quantity=5,
            unit_price=Decimal("9.99"),
            total_amount=Decimal("49.95"),
        )
        db_session.add(sale2)

        with pytest.raises(IntegrityError):
            await db_session.commit()

    async def test_foreign_key_constraint_enforced(self, db_session: AsyncSession):
        """Inserting with invalid foreign key should raise IntegrityError."""
        from datetime import date
        from decimal import Decimal

        sale = SalesDaily(
            date=date(2024, 1, 1),  # No calendar entry
            store_id=99999,  # Non-existent store
            product_id=99999,  # Non-existent product
            quantity=10,
            unit_price=Decimal("9.99"),
            total_amount=Decimal("99.90"),
        )
        db_session.add(sale)

        with pytest.raises(IntegrityError):
            await db_session.commit()

    async def test_check_constraint_quantity_positive(
        self, db_session: AsyncSession, sample_store: Store, sample_product: Product, sample_calendar: Calendar
    ):
        """Negative quantity should raise IntegrityError."""
        from decimal import Decimal

        sale = SalesDaily(
            date=sample_calendar.date,
            store_id=sample_store.id,
            product_id=sample_product.id,
            quantity=-5,  # Invalid: negative
            unit_price=Decimal("9.99"),
            total_amount=Decimal("99.90"),
        )
        db_session.add(sale)

        with pytest.raises(IntegrityError):
            await db_session.commit()
```

**File:** `app/features/data_platform/tests/conftest.py`

```python
"""Fixtures for data platform integration tests."""

from datetime import date
from decimal import Decimal

import pytest
from sqlalchemy.ext.asyncio import AsyncSession

from app.features.data_platform.models import Calendar, Product, Store


@pytest.fixture
async def sample_store(db_session: AsyncSession) -> Store:
    """Create a sample store for testing."""
    store = Store(
        code="TEST001",
        name="Test Store",
        region="Test Region",
        city="Test City",
        store_type="supermarket",
    )
    db_session.add(store)
    await db_session.commit()
    await db_session.refresh(store)
    return store


@pytest.fixture
async def sample_product(db_session: AsyncSession) -> Product:
    """Create a sample product for testing."""
    product = Product(
        sku="SKU-TEST-001",
        name="Test Product",
        category="Test Category",
        brand="Test Brand",
        base_price=Decimal("19.99"),
        base_cost=Decimal("9.99"),
    )
    db_session.add(product)
    await db_session.commit()
    await db_session.refresh(product)
    return product


@pytest.fixture
async def sample_calendar(db_session: AsyncSession) -> Calendar:
    """Create a sample calendar entry for testing."""
    calendar = Calendar(
        date=date(2024, 1, 15),
        day_of_week=0,  # Monday
        month=1,
        quarter=1,
        year=2024,
        is_holiday=False,
    )
    db_session.add(calendar)
    await db_session.commit()
    await db_session.refresh(calendar)
    return calendar
```

**Note:** Integration tests require `db_session` fixture in `tests/conftest.py` that provides actual database connection.

**Validation:**
```bash
# Unit tests (no DB required)
uv run pytest app/features/data_platform/tests/test_models.py -v

# Integration tests (requires DB)
docker-compose up -d
sleep 5
uv run alembic upgrade head
uv run pytest app/features/data_platform/tests/test_constraints.py -v -m integration
docker-compose down
```

---

### Task 9: Create example documentation files

**File:** `examples/schema/README.md`

```markdown
# ForecastLabAI Data Platform Schema

## Overview

The data platform implements a mini-warehouse schema optimized for retail demand forecasting.
It follows a star schema pattern with dimension and fact tables.

## Dimension Tables

### store
- **Primary Key**: `id` (surrogate)
- **Business Key**: `code` (unique)
- **Purpose**: Store locations and attributes

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER | Surrogate primary key |
| code | VARCHAR(20) | Unique store code |
| name | VARCHAR(100) | Store display name |
| region | VARCHAR(50) | Geographic region |
| city | VARCHAR(50) | City location |
| store_type | VARCHAR(30) | Store format |

### product
- **Primary Key**: `id` (surrogate)
- **Business Key**: `sku` (unique)
- **Purpose**: Product catalog

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER | Surrogate primary key |
| sku | VARCHAR(50) | Stock keeping unit |
| name | VARCHAR(200) | Product name |
| category | VARCHAR(100) | Product category |
| brand | VARCHAR(100) | Product brand |
| base_price | NUMERIC(10,2) | Standard retail price |
| base_cost | NUMERIC(10,2) | Standard cost/COGS |

### calendar
- **Primary Key**: `date` (natural key)
- **Purpose**: Time dimension for date-based analysis

| Column | Type | Description |
|--------|------|-------------|
| date | DATE | Calendar date (primary key) |
| day_of_week | INTEGER | 0=Monday, 6=Sunday |
| month | INTEGER | Month (1-12) |
| quarter | INTEGER | Quarter (1-4) |
| year | INTEGER | Year |
| is_holiday | BOOLEAN | Holiday flag |
| holiday_name | VARCHAR(100) | Holiday name |

## Fact Tables

### sales_daily (REQUIRED)
- **Grain**: One row per (date, store_id, product_id)
- **Purpose**: Daily aggregated sales transactions

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER | Surrogate primary key |
| date | DATE | Sales date (FK→calendar) |
| store_id | INTEGER | Store (FK→store) |
| product_id | INTEGER | Product (FK→product) |
| quantity | INTEGER | Units sold |
| unit_price | NUMERIC(10,2) | Price per unit |
| total_amount | NUMERIC(12,2) | Total sales amount |

**Critical Constraint**: `UNIQUE(date, store_id, product_id)` ensures grain protection
for idempotent upserts.

### price_history
- **Purpose**: Historical price tracking with validity windows

### promotion
- **Purpose**: Promotional campaigns with discount mechanics

### inventory_snapshot_daily
- **Grain**: One row per (date, store_id, product_id)
- **Purpose**: Daily inventory levels for stockout detection

## Index Strategy

Indexes are optimized for common forecasting query patterns:

1. **Time-range queries**: `ix_sales_daily_date_store`, `ix_sales_daily_date_product`
2. **Dimension lookups**: `ix_store_code`, `ix_product_sku`, `ix_product_category`
3. **Validity windows**: `ix_price_history_product_validity`

## Grain Protection

The `sales_daily` and `inventory_snapshot_daily` tables enforce grain via unique constraints.
This enables:
- **Idempotent upserts**: Re-running ingestion won't create duplicates
- **Data quality**: Prevents accidental double-counting
- **ON CONFLICT support**: PostgreSQL upsert pattern for replay-safe loading
```

**File:** `examples/queries/kpi_sales.sql`

```sql
-- ForecastLabAI KPI Query Examples
-- These queries demonstrate common analytical patterns

-- =============================================================================
-- Daily Sales Summary by Store
-- =============================================================================
SELECT
    s.date,
    st.code AS store_code,
    st.name AS store_name,
    COUNT(DISTINCT s.product_id) AS products_sold,
    SUM(s.quantity) AS total_units,
    SUM(s.total_amount) AS total_revenue
FROM sales_daily s
JOIN store st ON s.store_id = st.id
WHERE s.date BETWEEN '2024-01-01' AND '2024-01-31'
GROUP BY s.date, st.code, st.name
ORDER BY s.date, total_revenue DESC;

-- =============================================================================
-- Weekly Sales Trend by Category
-- =============================================================================
SELECT
    DATE_TRUNC('week', s.date) AS week_start,
    p.category,
    SUM(s.quantity) AS total_units,
    SUM(s.total_amount) AS total_revenue,
    AVG(s.unit_price) AS avg_price
FROM sales_daily s
JOIN product p ON s.product_id = p.id
WHERE s.date >= CURRENT_DATE - INTERVAL '12 weeks'
GROUP BY DATE_TRUNC('week', s.date), p.category
ORDER BY week_start, p.category;

-- =============================================================================
-- Top 10 Products by Revenue (Last 30 Days)
-- =============================================================================
SELECT
    p.sku,
    p.name,
    p.category,
    SUM(s.quantity) AS total_units,
    SUM(s.total_amount) AS total_revenue,
    RANK() OVER (ORDER BY SUM(s.total_amount) DESC) AS revenue_rank
FROM sales_daily s
JOIN product p ON s.product_id = p.id
WHERE s.date >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY p.sku, p.name, p.category
ORDER BY total_revenue DESC
LIMIT 10;

-- =============================================================================
-- Year-over-Year Growth by Store
-- =============================================================================
WITH current_year AS (
    SELECT
        store_id,
        SUM(total_amount) AS revenue
    FROM sales_daily
    WHERE date >= DATE_TRUNC('year', CURRENT_DATE)
    GROUP BY store_id
),
prior_year AS (
    SELECT
        store_id,
        SUM(total_amount) AS revenue
    FROM sales_daily
    WHERE date >= DATE_TRUNC('year', CURRENT_DATE) - INTERVAL '1 year'
      AND date < DATE_TRUNC('year', CURRENT_DATE)
    GROUP BY store_id
)
SELECT
    st.code AS store_code,
    st.name AS store_name,
    cy.revenue AS current_year_revenue,
    py.revenue AS prior_year_revenue,
    ROUND((cy.revenue - py.revenue) / NULLIF(py.revenue, 0) * 100, 2) AS yoy_growth_pct
FROM current_year cy
JOIN prior_year py ON cy.store_id = py.store_id
JOIN store st ON cy.store_id = st.id
ORDER BY yoy_growth_pct DESC;
```

**File:** `examples/queries/exog_join.sql`

```sql
-- ForecastLabAI Exogenous Feature Join Examples
-- Join patterns for sales + price/promo/inventory signals

-- =============================================================================
-- Sales with Active Price (Point-in-Time Join)
-- =============================================================================
SELECT
    s.date,
    s.store_id,
    s.product_id,
    s.quantity,
    s.unit_price AS sale_price,
    ph.price AS list_price,
    CASE
        WHEN ph.price > 0 THEN
            ROUND((ph.price - s.unit_price) / ph.price * 100, 2)
        ELSE 0
    END AS discount_pct
FROM sales_daily s
LEFT JOIN price_history ph ON
    ph.product_id = s.product_id
    AND (ph.store_id IS NULL OR ph.store_id = s.store_id)
    AND s.date >= ph.valid_from
    AND (ph.valid_to IS NULL OR s.date <= ph.valid_to)
WHERE s.date BETWEEN '2024-01-01' AND '2024-01-31';

-- =============================================================================
-- Sales with Active Promotions
-- =============================================================================
SELECT
    s.date,
    s.store_id,
    s.product_id,
    s.quantity,
    s.total_amount,
    pr.name AS promo_name,
    pr.discount_pct AS promo_discount_pct,
    pr.discount_amount AS promo_discount_amount,
    CASE WHEN pr.id IS NOT NULL THEN TRUE ELSE FALSE END AS on_promotion
FROM sales_daily s
LEFT JOIN promotion pr ON
    pr.product_id = s.product_id
    AND (pr.store_id IS NULL OR pr.store_id = s.store_id)
    AND s.date BETWEEN pr.start_date AND pr.end_date
WHERE s.date BETWEEN '2024-01-01' AND '2024-01-31';

-- =============================================================================
-- Sales with Inventory Signals (Stockout Detection)
-- =============================================================================
SELECT
    s.date,
    s.store_id,
    s.product_id,
    s.quantity AS units_sold,
    inv.on_hand_qty AS eod_inventory,
    inv.is_stockout,
    CASE
        WHEN inv.on_hand_qty < s.quantity * 2 THEN 'LOW'
        WHEN inv.on_hand_qty < s.quantity * 7 THEN 'MEDIUM'
        ELSE 'OK'
    END AS inventory_status
FROM sales_daily s
LEFT JOIN inventory_snapshot_daily inv ON
    inv.date = s.date
    AND inv.store_id = s.store_id
    AND inv.product_id = s.product_id
WHERE s.date BETWEEN '2024-01-01' AND '2024-01-31';

-- =============================================================================
-- Full Feature Set for Forecasting (All Exogenous Signals)
-- =============================================================================
SELECT
    s.date,
    st.code AS store_code,
    st.region,
    st.store_type,
    p.sku,
    p.category,
    p.brand,
    c.day_of_week,
    c.month,
    c.quarter,
    c.is_holiday,
    s.quantity,
    s.unit_price,
    s.total_amount,
    ph.price AS list_price,
    COALESCE(pr.discount_pct, 0) AS promo_discount_pct,
    CASE WHEN pr.id IS NOT NULL THEN 1 ELSE 0 END AS on_promotion,
    inv.on_hand_qty,
    inv.is_stockout::INT AS stockout_flag
FROM sales_daily s
-- Dimension joins
JOIN store st ON s.store_id = st.id
JOIN product p ON s.product_id = p.id
JOIN calendar c ON s.date = c.date
-- Exogenous signal joins
LEFT JOIN price_history ph ON
    ph.product_id = s.product_id
    AND (ph.store_id IS NULL OR ph.store_id = s.store_id)
    AND s.date >= ph.valid_from
    AND (ph.valid_to IS NULL OR s.date <= ph.valid_to)
LEFT JOIN promotion pr ON
    pr.product_id = s.product_id
    AND (pr.store_id IS NULL OR pr.store_id = s.store_id)
    AND s.date BETWEEN pr.start_date AND pr.end_date
LEFT JOIN inventory_snapshot_daily inv ON
    inv.date = s.date
    AND inv.store_id = s.store_id
    AND inv.product_id = s.product_id
WHERE s.date BETWEEN '2024-01-01' AND '2024-01-31'
ORDER BY s.date, st.code, p.sku;
```

**Validation:**
```bash
ls -la examples/schema/
ls -la examples/queries/
```

---

### Task 10: Update tests/conftest.py with database session fixture

**File:** `tests/conftest.py`

Add `db_session` fixture for integration tests:

```python
"""Shared pytest fixtures for ForecastLabAI tests."""

import pytest
from httpx import ASGITransport, AsyncClient
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine

from app.core.config import get_settings
from app.core.database import Base
from app.main import app


@pytest.fixture
async def client():
    """Create async HTTP client for testing FastAPI endpoints."""
    async with AsyncClient(
        transport=ASGITransport(app=app),
        base_url="http://test",
    ) as ac:
        yield ac


@pytest.fixture
async def db_session():
    """Create async database session for integration tests.

    This fixture creates all tables, provides a session, and cleans up after.
    Requires PostgreSQL to be running (docker-compose up -d).
    """
    settings = get_settings()
    engine = create_async_engine(settings.database_url, echo=False)

    # Create tables
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    # Create session
    async_session_maker = async_sessionmaker(
        engine,
        class_=AsyncSession,
        expire_on_commit=False,
    )

    async with async_session_maker() as session:
        try:
            yield session
        finally:
            await session.rollback()

    # Cleanup: drop all tables
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)

    await engine.dispose()
```

**Validation:**
```bash
uv run pytest tests/conftest.py --collect-only
```

---

### Task 11: Final validation - Run all quality gates

```bash
# Format and lint
uv run ruff check app/features/data_platform/ --fix
uv run ruff format app/features/data_platform/

# Type checking
uv run mypy app/features/data_platform/
uv run pyright app/features/data_platform/

# Unit tests (no DB required)
uv run pytest app/features/data_platform/tests/test_models.py -v

# Integration tests (requires DB)
docker-compose up -d
sleep 5
uv run alembic upgrade head
uv run pytest app/features/data_platform/tests/ -v -m integration
docker-compose down

# Full test suite
uv run pytest -v
```

---

## Validation Loop

### Level 1: Syntax & Style

```bash
# Run FIRST - fix any errors before proceeding
uv run ruff check app/features/data_platform/ --fix
uv run ruff format app/features/data_platform/

# Expected: No errors
```

### Level 2: Type Checking

```bash
# Run SECOND - type safety is non-negotiable
uv run mypy app/features/data_platform/
uv run pyright app/features/data_platform/

# Expected: 0 errors, 0 warnings
```

### Level 3: Unit Tests

```bash
# Run THIRD - verify model definitions
uv run pytest app/features/data_platform/tests/test_models.py -v

# Expected: All tests pass
```

### Level 4: Migration Test

```bash
# Run FOURTH - verify migrations work
docker-compose up -d
sleep 5
uv run alembic upgrade head
uv run alembic downgrade -1
uv run alembic upgrade head
docker-compose down

# Expected: No errors
```

### Level 5: Integration Tests

```bash
# Run FIFTH - verify constraints work in real DB
docker-compose up -d
sleep 5
uv run alembic upgrade head
uv run pytest app/features/data_platform/tests/test_constraints.py -v -m integration
docker-compose down

# Expected: All tests pass
```

---

## Final Validation Checklist

- [ ] `uv run ruff check app/features/data_platform/` passes with no errors
- [ ] `uv run ruff format --check app/features/data_platform/` passes
- [ ] `uv run mypy app/features/data_platform/` passes with 0 errors
- [ ] `uv run pyright app/features/data_platform/` passes with 0 errors
- [ ] `uv run pytest app/features/data_platform/tests/test_models.py -v` all tests pass
- [ ] `uv run alembic upgrade head` creates all tables
- [ ] `uv run alembic downgrade -1 && uv run alembic upgrade head` works
- [ ] Migration file has proper `downgrade()` function
- [ ] All unique constraints have proper names (uq_*)
- [ ] All indexes have proper names (ix_*)
- [ ] All check constraints have proper names (ck_*)
- [ ] `examples/schema/README.md` documents all tables
- [ ] `examples/queries/kpi_sales.sql` contains working queries
- [ ] `examples/queries/exog_join.sql` contains join patterns

---

## Integration Points

```yaml
DATABASE:
  - migration: "0001_create_data_platform_tables.py"
  - tables: store, product, calendar, sales_daily, price_history, promotion, inventory_snapshot_daily

CONFIG:
  - no new settings required
  - uses existing DATABASE_URL from app.core.config

IMPORTS:
  - update alembic/env.py to import data_platform.models
  - models automatically included via Base.metadata

TESTS:
  - add db_session fixture to tests/conftest.py for integration tests
```

---

## Anti-Patterns to Avoid

- ❌ **Don't** use `Column()` - use `mapped_column()` (SQLAlchemy 2.0)
- ❌ **Don't** use float for money - use `Decimal` via `Numeric(10, 2)`
- ❌ **Don't** skip type annotations - every column needs `Mapped[type]`
- ❌ **Don't** create index for unique constraint columns (PostgreSQL auto-creates)
- ❌ **Don't** use anonymous constraints - always provide names
- ❌ **Don't** forget `back_populates` on relationships
- ❌ **Don't** reference class names in ForeignKey - use table names
- ❌ **Don't** skip integration tests for constraints - they catch real bugs

---

## Confidence Score: 9/10

**Rationale:**
- (+) Complete model definitions with all columns, types, and constraints
- (+) Explicit unique constraint for grain protection (critical requirement)
- (+) Comprehensive index strategy for query performance
- (+) Full Pydantic schemas for API validation
- (+) Unit tests for model structure
- (+) Integration tests for constraint enforcement
- (+) Example documentation and SQL queries
- (+) All gotchas explicitly documented
- (+) Follows existing codebase patterns (TimestampMixin, Base)
- (-) Alembic autogenerate may need manual review
- (-) Integration tests require running database

**Recommended Approach:**
1. Execute tasks 1-3 (create directory, implement models)
2. Run type checkers after each file
3. Execute task 4-5 (update alembic, generate migration)
4. Review generated migration manually
5. Execute tasks 6-9 (schemas, tests, examples)
6. Run full validation loop

---

## Version

- **PRP Version:** 1.0
- **Target INITIAL:** INITIAL-2.md (Data Platform: Schema + Migrations)
- **Created:** 2026-01-26
- **Author:** Claude Code

---

## References

### SQLAlchemy 2.0
- [ORM Quick Start](https://docs.sqlalchemy.org/en/20/orm/quickstart.html)
- [ORM Mapped Class Configuration](https://docs.sqlalchemy.org/en/20/orm/mapper_config.html)
- [Schema Definition Language](https://docs.sqlalchemy.org/en/20/core/schema.html)

### Alembic
- [Operation Reference](https://alembic.sqlalchemy.org/en/latest/ops.html)
- [Auto Generating Migrations](https://alembic.sqlalchemy.org/en/latest/autogenerate.html)
- [Naming Conventions](https://alembic.sqlalchemy.org/en/latest/naming.html)

### Retail Forecasting
- [Retail Demand Forecasting with SQL and Python](https://www.analyticsvidhya.com/blog/2025/10/retail-demand-forecasting/)
- [Data Warehouse SQL Retail Sales (GitHub)](https://github.com/saiful-islam-rupom/data-warehouse-sql-retail-sales)

### Medium Articles
- [Modern SQLAlchemy 2.0 Patterns](https://medium.com/@azizmarzouki/embracing-modern-sqlalchemy-2-0-declarativebase-mapped-and-beyond-ef8bcba1e79c)
- [FastAPI with Async SQLAlchemy 2.0](https://medium.com/@tclaitken/setting-up-a-fastapi-app-with-async-sqlalchemy-2-0-pydantic-v2-e6c540be4308)
